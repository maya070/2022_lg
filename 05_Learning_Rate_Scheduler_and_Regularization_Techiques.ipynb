{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1CutxEYpxNPHrHH6o39WV4098THLS5wOK","timestamp":1627397397565}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"9IuOTnU2Gevg"},"source":["#  Study Learning Rate Scheduler and Regularization Techiques\n","#### Learning Rate Scheduler의 Pytorch module인 optim.lr_scheduler와 Early stopping, Dropout, Batch Normalization, Weight decay와 같은 Regularization Techiques에 대해 실습을 할 계획입니다."]},{"cell_type":"markdown","metadata":{"id":"gtctFVNgdp6q"},"source":["# Learning Rate Scheduler"]},{"cell_type":"markdown","metadata":{"id":"xTsr_TAkKmWK"},"source":["## 1) Learning Rate Scheduler : optim.lr_scheduler\n"," - [doc] (https://pytorch.org/docs/stable/optim.html)"]},{"cell_type":"code","metadata":{"id":"WyWiElwZ-L0u"},"source":["# 패키지 import\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","from torchsummary import summary"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2gyzM_OF0Zw5"},"source":["# Model 설정\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.relu = nn.ReLU()\n","        self.fc1 = nn.Linear(784, 256)\n","        self.fc2 = nn.Linear(256, 128)\n","        self.fc3 = nn.Linear(128, 10)\n","    \n","    def forward(self, x):\n","        x = x.view(-1, 784)\n","        x = nn.relu(self.fc1(x))\n","        x = nn.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n","model = Net()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QgLv3VK8K6zK"},"source":["# Parameter 설정\n","########################################## Change This Code~!\n","EPOCHS = 100\n","LR = 0.01\n","STEP_SIZE = 30\n","GAMMA = 0.1\n","########################################## Change This Code~!\n","\n","# Optimizer 및 Scheduler 설정\n","optimizer = optim.SGD(model.parameters(), lr=LR)\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA) # step size마다 gamma 비율로 lr을 감소시킨다. (step_size 마다 gamma를 곱한다.)\n","\n","def get_lr(optimizer, scheduler): # Training 코드\n","    lr = scheduler.get_last_lr()[0]\n","    optimizer.step()\n","    scheduler.step() # Scheduler를 사용한다면 필수적으로 들어가야함\n","    return lr\n","\n","plt.plot(torch.arange(EPOCHS), [get_lr(optimizer, scheduler) for t in range(EPOCHS)])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5sCDot73ctvA"},"source":["# Regularization Techniques"]},{"cell_type":"markdown","metadata":{"id":"xlzoACw9eOnw"},"source":["## 1) Early Stopping\n"]},{"cell_type":"code","metadata":{"id":"0nQimUqQxGGH"},"source":["# Early Stopping Algorithm\n","\n","# valid_loader를 만들어 보세요!\n","\n","count = 0\n","valid_acc = 0\n","for epcho in range(EPOCHS): # training loop\n","    for idx, (image, target) in enumerate(valid_loader): # at each epoch\n","        output = model(image) # model inference\n","        valid_acc = accuracy(output, target) # calculate valid accuracy\n","        if valid_acc > best_valid_acc: # Valid dataset에 대한 accuracy가 계속 증가하고 있으면 --> count=0, model 저장\n","            best_valid_acc = valid_acc\n","            torch.save(model.state_dict(), path) # save best model\n","            count = 0\n","        else: # Valid Accuracy가 증가하지 않을 때 \n","            count += 1\n","            if count >= 3: # 연속 3번 epoch 동안 accuracy가 best accuracy보다 크지 못하다면\n","                break # Early Stopping Point 이다.\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7f7jg-aqd_hq"},"source":["## 2) Batch Normalization : nn.BatchNorm2d\n","\n","- [doc] (https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html)"]},{"cell_type":"code","metadata":{"id":"dsK1rtKJH-SB"},"source":["# Model 설정\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.relu = nn.ReLU()\n","        self.fc1 = nn.Linear(784, 256)\n","        self.bn1 = nn.BatchNorm1d(num_features=256) # x가 2-dim 이면 nn.BatchNorm2d\n","        self.fc2 = nn.Linear(256, 128)\n","        self.bn2 = nn.BatchNorm1d(num_features=128) # x가 2-dim 이면 nn.BatchNorm2d\n","        self.fc3 = nn.Linear(128, 10)\n","    \n","    def forward(self, x):\n","        x = x.view(-1, 784)\n","        ########################################## Change This Code~!\n","        x = self.relu(self.bn1(self.fc1(x)))\n","        x = self.relu(self.bn2(self.fc2(x)))\n","        ########################################## Change This Code~!\n","        x = self.fc3(x)\n","        return x\n","\n","model = Net().to(\"cuda\")\n","summary(model, (1, 28, 28))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AtgtyCq7eck2"},"source":["## 3) Dropout : nn.Dropout2d\n"," - [doc] (https://pytorch.org/docs/stable/generated/torch.nn.Dropout2d.html)"]},{"cell_type":"code","metadata":{"id":"s2myTmx9JMhN"},"source":["# Model 설정\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.dropout = nn.Dropout(p=0.5) # x가 2-dim 이면 nn.Dropout2d\n","        self.relu = nn.ReLU()\n","        self.fc1 = nn.Linear(784, 256)\n","        self.bn1 = nn.BatchNorm1d(256) # x가 2-dim 이면 nn.BatchNorm2d\n","        self.fc2 = nn.Linear(256, 128)\n","        self.bn2 = nn.BatchNorm1d(128) # x가 2-dim 이면 nn.BatchNorm2d\n","        self.fc3 = nn.Linear(128, 10)\n","    \n","    def forward(self, x):\n","        x = x.view(-1, 784)\n","        ########################################## Change This Code~!\n","        x = self.dropout(self.relu(self.bn1(self.fc1(x))))\n","        x = self.dropout(self.relu(self.bn2(self.fc2(x))))\n","        ########################################## Change This Code~!\n","        x = self.fc3(x)\n","        return x\n","\n","model = Net().to(\"cuda\")\n","summary(model, (1, 28, 28))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g5oOJVdye-HY"},"source":["## 4) Weight Decay : optim.METHOD(weight_decay=1e-5)\n","\n"," - [doc] (https://pytorch.org/docs/stable/optim.html)\n","\n","  데이터가 단순하고 모델이 복잡하면, 학습을 하면서 굉장히 작은 값이었던 weight들의 값이 점점 증가하게 되면서 Overfitting이 발생하게 됨.\n","\n","  Pytorch에서는 L2 Regularization Term을 optim.METHOD(weight_decay=lambda) 와 같이 사용할 수 있음.\n","\n"]},{"cell_type":"code","metadata":{"id":"UAdImLV3QMTy"},"source":["# Optimizer 설정\n","########################################## Change This Code~!\n","Lambda = 1e-6\n","########################################## Change This Code~!\n","optimizer = optim.SGD(model.parameters(), lr=LR, weight_decay=Lambda)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_TxXFVcbwCKE"},"source":[],"execution_count":null,"outputs":[]}]}